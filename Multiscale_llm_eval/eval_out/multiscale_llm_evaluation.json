{
  "score_thresholds": {
    "expert": 4.0,
    "competent": 3.0,
    "developing": 2.0
  },
  "benchmark_mapping": {
    "gsm8k": "math_reasoning",
    "hellaswag": "commonsense",
    "boolq": "reading",
    "mmlu.humanities": "humanities",
    "mmlu.social_sciences": "social_sciences",
    "mmlu.stem": "stem",
    "mmlu.other": "other",
    "mmlu": "mmlu_overall",
    "^gsm8k$": "math_reasoning",
    "^(Rowan/)?hellaswag$": "commonsense",
    "^(super_glue/)?boolq$": "reading_comprehension",
    "^mmlu$": "general_knowledge",
    "^mmlu_humanities$": "humanities",
    "^mmlu_social_sciences$": "social_science",
    "^mmlu_stem$": "stem",
    "^mmlu_other$": "other_knowledge"
  },
  "models_meta": {
    "qwen/Qwen2.5-1.5B-Instruct": {
      "parameters": "",
      "family": "Qwen",
      "release_date": "",
      "extras": {}
    },
    "deepseek-ai/DeepSeek-Coder-1.3B-Instruct": {
      "parameters": "",
      "family": "DeepSeek",
      "release_date": "",
      "extras": {}
    },
    "qwen/Qwen2.5-3B-Instruct": {
      "parameters": "",
      "family": "Qwen",
      "release_date": "",
      "extras": {}
    }
  },
  "models_raw_scores": {
    "qwen/Qwen2.5-3B-Instruct": {
      "boolq": 77.0,
      "gsm8k": 62.0,
      "hellaswag": 52.0,
      "mmlu": 65.89473684210526,
      "mmlu_humanities": 69.3076923076923,
      "mmlu_formal_logic": 45.0,
      "mmlu_high_school_european_history": 75.0,
      "mmlu_high_school_us_history": 86.0,
      "mmlu_high_school_world_history": 80.0,
      "mmlu_international_law": 76.0,
      "mmlu_jurisprudence": 80.0,
      "mmlu_logical_fallacies": 77.0,
      "mmlu_moral_disputes": 64.0,
      "mmlu_moral_scenarios": 40.0,
      "mmlu_philosophy": 73.0,
      "mmlu_prehistory": 76.0,
      "mmlu_professional_law": 48.0,
      "mmlu_world_religions": 81.0,
      "mmlu_other": 68.15384615384616,
      "mmlu_business_ethics": 70.0,
      "mmlu_clinical_knowledge": 69.0,
      "mmlu_college_medicine": 68.0,
      "mmlu_global_facts": 40.0,
      "mmlu_human_aging": 65.0,
      "mmlu_management": 78.0,
      "mmlu_marketing": 85.0,
      "mmlu_medical_genetics": 78.0,
      "mmlu_miscellaneous": 82.0,
      "mmlu_nutrition": 78.0,
      "mmlu_professional_accounting": 49.0,
      "mmlu_professional_medicine": 71.0,
      "mmlu_virology": 53.0,
      "mmlu_social_sciences": 74.58333333333333,
      "mmlu_econometrics": 56.99999999999999,
      "mmlu_high_school_geography": 74.0,
      "mmlu_high_school_government_and_politics": 86.0,
      "mmlu_high_school_macroeconomics": 74.0,
      "mmlu_high_school_microeconomics": 76.0,
      "mmlu_high_school_psychology": 84.0,
      "mmlu_human_sexuality": 73.0,
      "mmlu_professional_psychology": 66.0,
      "mmlu_public_relations": 68.0,
      "mmlu_security_studies": 79.0,
      "mmlu_sociology": 76.0,
      "mmlu_us_foreign_policy": 82.0,
      "mmlu_stem": 56.52631578947368,
      "mmlu_abstract_algebra": 40.0,
      "mmlu_anatomy": 61.0,
      "mmlu_astronomy": 70.0,
      "mmlu_college_biology": 73.0,
      "mmlu_college_chemistry": 47.0,
      "mmlu_college_computer_science": 52.0,
      "mmlu_college_mathematics": 35.0,
      "mmlu_college_physics": 45.0,
      "mmlu_computer_security": 71.0,
      "mmlu_conceptual_physics": 62.0,
      "mmlu_electrical_engineering": 62.0,
      "mmlu_elementary_mathematics": 63.0,
      "mmlu_high_school_biology": 77.0,
      "mmlu_high_school_chemistry": 61.0,
      "mmlu_high_school_computer_science": 71.0,
      "mmlu_high_school_mathematics": 40.0,
      "mmlu_high_school_physics": 43.0,
      "mmlu_high_school_statistics": 56.00000000000001,
      "mmlu_machine_learning": 45.0
    },
    "deepseek-ai/DeepSeek-Coder-1.3B-Instruct": {
      "boolq": 71.0,
      "gsm8k": 8.0,
      "hellaswag": 36.0,
      "mmlu": 23.807017543859647,
      "mmlu_humanities": 25.076923076923073,
      "mmlu_formal_logic": 27.0,
      "mmlu_high_school_european_history": 23.0,
      "mmlu_high_school_us_history": 25.0,
      "mmlu_high_school_world_history": 30.0,
      "mmlu_international_law": 25.0,
      "mmlu_jurisprudence": 28.000000000000004,
      "mmlu_logical_fallacies": 24.0,
      "mmlu_moral_disputes": 22.0,
      "mmlu_moral_scenarios": 22.0,
      "mmlu_philosophy": 15.0,
      "mmlu_prehistory": 23.0,
      "mmlu_professional_law": 22.0,
      "mmlu_world_religions": 40.0,
      "mmlu_other": 22.230769230769234,
      "mmlu_business_ethics": 32.0,
      "mmlu_clinical_knowledge": 15.0,
      "mmlu_college_medicine": 18.0,
      "mmlu_global_facts": 18.0,
      "mmlu_human_aging": 28.999999999999996,
      "mmlu_management": 16.0,
      "mmlu_marketing": 28.999999999999996,
      "mmlu_medical_genetics": 28.999999999999996,
      "mmlu_miscellaneous": 19.0,
      "mmlu_nutrition": 21.0,
      "mmlu_professional_accounting": 21.0,
      "mmlu_professional_medicine": 15.0,
      "mmlu_virology": 27.0,
      "mmlu_social_sciences": 23.166666666666664,
      "mmlu_econometrics": 21.0,
      "mmlu_high_school_geography": 16.0,
      "mmlu_high_school_government_and_politics": 16.0,
      "mmlu_high_school_macroeconomics": 14.000000000000002,
      "mmlu_high_school_microeconomics": 26.0,
      "mmlu_high_school_psychology": 23.0,
      "mmlu_human_sexuality": 23.0,
      "mmlu_professional_psychology": 27.0,
      "mmlu_public_relations": 24.0,
      "mmlu_security_studies": 27.0,
      "mmlu_sociology": 33.0,
      "mmlu_us_foreign_policy": 28.000000000000004,
      "mmlu_stem": 24.421052631578945,
      "mmlu_abstract_algebra": 22.0,
      "mmlu_anatomy": 17.0,
      "mmlu_astronomy": 20.0,
      "mmlu_college_biology": 24.0,
      "mmlu_college_chemistry": 20.0,
      "mmlu_college_computer_science": 36.0,
      "mmlu_college_mathematics": 22.0,
      "mmlu_college_physics": 20.0,
      "mmlu_computer_security": 34.0,
      "mmlu_conceptual_physics": 33.0,
      "mmlu_electrical_engineering": 28.999999999999996,
      "mmlu_elementary_mathematics": 26.0,
      "mmlu_high_school_biology": 15.0,
      "mmlu_high_school_chemistry": 18.0,
      "mmlu_high_school_computer_science": 28.999999999999996,
      "mmlu_high_school_mathematics": 23.0,
      "mmlu_high_school_physics": 23.0,
      "mmlu_high_school_statistics": 21.0,
      "mmlu_machine_learning": 32.0
    },
    "qwen/Qwen2.5-1.5B-Instruct": {
      "boolq": 79.0,
      "gsm8k": 56.99999999999999,
      "hellaswag": 47.0,
      "mmlu": 59.175438596491226,
      "mmlu_humanities": 63.92307692307693,
      "mmlu_formal_logic": 47.0,
      "mmlu_high_school_european_history": 73.0,
      "mmlu_high_school_us_history": 74.0,
      "mmlu_high_school_world_history": 76.0,
      "mmlu_international_law": 75.0,
      "mmlu_jurisprudence": 74.0,
      "mmlu_logical_fallacies": 78.0,
      "mmlu_moral_disputes": 64.0,
      "mmlu_moral_scenarios": 28.000000000000004,
      "mmlu_philosophy": 65.0,
      "mmlu_prehistory": 64.0,
      "mmlu_professional_law": 39.0,
      "mmlu_world_religions": 74.0,
      "mmlu_other": 62.0,
      "mmlu_business_ethics": 61.0,
      "mmlu_clinical_knowledge": 68.0,
      "mmlu_college_medicine": 64.0,
      "mmlu_global_facts": 26.0,
      "mmlu_human_aging": 66.0,
      "mmlu_management": 76.0,
      "mmlu_marketing": 86.0,
      "mmlu_medical_genetics": 67.0,
      "mmlu_miscellaneous": 64.0,
      "mmlu_nutrition": 69.0,
      "mmlu_professional_accounting": 49.0,
      "mmlu_professional_medicine": 66.0,
      "mmlu_virology": 44.0,
      "mmlu_social_sciences": 68.75,
      "mmlu_econometrics": 47.0,
      "mmlu_high_school_geography": 78.0,
      "mmlu_high_school_government_and_politics": 79.0,
      "mmlu_high_school_macroeconomics": 65.0,
      "mmlu_high_school_microeconomics": 71.0,
      "mmlu_high_school_psychology": 83.0,
      "mmlu_human_sexuality": 71.0,
      "mmlu_professional_psychology": 60.0,
      "mmlu_public_relations": 62.0,
      "mmlu_security_studies": 62.0,
      "mmlu_sociology": 74.0,
      "mmlu_us_foreign_policy": 73.0,
      "mmlu_stem": 47.94736842105263,
      "mmlu_abstract_algebra": 26.0,
      "mmlu_anatomy": 51.0,
      "mmlu_astronomy": 68.0,
      "mmlu_college_biology": 63.0,
      "mmlu_college_chemistry": 37.0,
      "mmlu_college_computer_science": 50.0,
      "mmlu_college_mathematics": 28.999999999999996,
      "mmlu_college_physics": 38.0,
      "mmlu_computer_security": 72.0,
      "mmlu_conceptual_physics": 56.99999999999999,
      "mmlu_electrical_engineering": 55.00000000000001,
      "mmlu_elementary_mathematics": 37.0,
      "mmlu_high_school_biology": 70.0,
      "mmlu_high_school_chemistry": 44.0,
      "mmlu_high_school_computer_science": 60.0,
      "mmlu_high_school_mathematics": 33.0,
      "mmlu_high_school_physics": 31.0,
      "mmlu_high_school_statistics": 43.0,
      "mmlu_machine_learning": 47.0
    }
  },
  "models_dim_scores": {
    "qwen/Qwen2.5-3B-Instruct": {
      "reading": 77.0,
      "math_reasoning": 62.0,
      "commonsense": 52.0,
      "mmlu_overall": 65.89473684210526,
      "humanities": 69.3076923076923,
      "mmlu.formal_logic": 45.0,
      "mmlu.high_school_european_history": 75.0,
      "mmlu.high_school_us_history": 86.0,
      "mmlu.high_school_world_history": 80.0,
      "mmlu.international_law": 76.0,
      "mmlu.jurisprudence": 80.0,
      "mmlu.logical_fallacies": 77.0,
      "mmlu.moral_disputes": 64.0,
      "mmlu.moral_scenarios": 40.0,
      "mmlu.philosophy": 73.0,
      "mmlu.prehistory": 76.0,
      "mmlu.professional_law": 48.0,
      "mmlu.world_religions": 81.0,
      "other": 68.15384615384616,
      "mmlu.business_ethics": 70.0,
      "mmlu.clinical_knowledge": 69.0,
      "mmlu.college_medicine": 68.0,
      "mmlu.global_facts": 40.0,
      "mmlu.human_aging": 65.0,
      "mmlu.management": 78.0,
      "mmlu.marketing": 85.0,
      "mmlu.medical_genetics": 78.0,
      "mmlu.miscellaneous": 82.0,
      "mmlu.nutrition": 78.0,
      "mmlu.professional_accounting": 49.0,
      "mmlu.professional_medicine": 71.0,
      "mmlu.virology": 53.0,
      "social_sciences": 74.58333333333333,
      "mmlu.econometrics": 56.99999999999999,
      "mmlu.high_school_geography": 74.0,
      "mmlu.high_school_government_and_politics": 86.0,
      "mmlu.high_school_macroeconomics": 74.0,
      "mmlu.high_school_microeconomics": 76.0,
      "mmlu.high_school_psychology": 84.0,
      "mmlu.human_sexuality": 73.0,
      "mmlu.professional_psychology": 66.0,
      "mmlu.public_relations": 68.0,
      "mmlu.security_studies": 79.0,
      "mmlu.sociology": 76.0,
      "mmlu.us_foreign_policy": 82.0,
      "stem": 56.52631578947368,
      "mmlu.abstract_algebra": 40.0,
      "mmlu.anatomy": 61.0,
      "mmlu.astronomy": 70.0,
      "mmlu.college_biology": 73.0,
      "mmlu.college_chemistry": 47.0,
      "mmlu.college_computer_science": 52.0,
      "mmlu.college_mathematics": 35.0,
      "mmlu.college_physics": 45.0,
      "mmlu.computer_security": 71.0,
      "mmlu.conceptual_physics": 62.0,
      "mmlu.electrical_engineering": 62.0,
      "mmlu.elementary_mathematics": 63.0,
      "mmlu.high_school_biology": 77.0,
      "mmlu.high_school_chemistry": 61.0,
      "mmlu.high_school_computer_science": 71.0,
      "mmlu.high_school_mathematics": 40.0,
      "mmlu.high_school_physics": 43.0,
      "mmlu.high_school_statistics": 56.00000000000001,
      "mmlu.machine_learning": 45.0
    },
    "deepseek-ai/DeepSeek-Coder-1.3B-Instruct": {
      "reading": 71.0,
      "math_reasoning": 8.0,
      "commonsense": 36.0,
      "mmlu_overall": 23.807017543859647,
      "humanities": 25.076923076923073,
      "mmlu.formal_logic": 27.0,
      "mmlu.high_school_european_history": 23.0,
      "mmlu.high_school_us_history": 25.0,
      "mmlu.high_school_world_history": 30.0,
      "mmlu.international_law": 25.0,
      "mmlu.jurisprudence": 28.000000000000004,
      "mmlu.logical_fallacies": 24.0,
      "mmlu.moral_disputes": 22.0,
      "mmlu.moral_scenarios": 22.0,
      "mmlu.philosophy": 15.0,
      "mmlu.prehistory": 23.0,
      "mmlu.professional_law": 22.0,
      "mmlu.world_religions": 40.0,
      "other": 22.230769230769234,
      "mmlu.business_ethics": 32.0,
      "mmlu.clinical_knowledge": 15.0,
      "mmlu.college_medicine": 18.0,
      "mmlu.global_facts": 18.0,
      "mmlu.human_aging": 28.999999999999996,
      "mmlu.management": 16.0,
      "mmlu.marketing": 28.999999999999996,
      "mmlu.medical_genetics": 28.999999999999996,
      "mmlu.miscellaneous": 19.0,
      "mmlu.nutrition": 21.0,
      "mmlu.professional_accounting": 21.0,
      "mmlu.professional_medicine": 15.0,
      "mmlu.virology": 27.0,
      "social_sciences": 23.166666666666664,
      "mmlu.econometrics": 21.0,
      "mmlu.high_school_geography": 16.0,
      "mmlu.high_school_government_and_politics": 16.0,
      "mmlu.high_school_macroeconomics": 14.000000000000002,
      "mmlu.high_school_microeconomics": 26.0,
      "mmlu.high_school_psychology": 23.0,
      "mmlu.human_sexuality": 23.0,
      "mmlu.professional_psychology": 27.0,
      "mmlu.public_relations": 24.0,
      "mmlu.security_studies": 27.0,
      "mmlu.sociology": 33.0,
      "mmlu.us_foreign_policy": 28.000000000000004,
      "stem": 24.421052631578945,
      "mmlu.abstract_algebra": 22.0,
      "mmlu.anatomy": 17.0,
      "mmlu.astronomy": 20.0,
      "mmlu.college_biology": 24.0,
      "mmlu.college_chemistry": 20.0,
      "mmlu.college_computer_science": 36.0,
      "mmlu.college_mathematics": 22.0,
      "mmlu.college_physics": 20.0,
      "mmlu.computer_security": 34.0,
      "mmlu.conceptual_physics": 33.0,
      "mmlu.electrical_engineering": 28.999999999999996,
      "mmlu.elementary_mathematics": 26.0,
      "mmlu.high_school_biology": 15.0,
      "mmlu.high_school_chemistry": 18.0,
      "mmlu.high_school_computer_science": 28.999999999999996,
      "mmlu.high_school_mathematics": 23.0,
      "mmlu.high_school_physics": 23.0,
      "mmlu.high_school_statistics": 21.0,
      "mmlu.machine_learning": 32.0
    },
    "qwen/Qwen2.5-1.5B-Instruct": {
      "reading": 79.0,
      "math_reasoning": 56.99999999999999,
      "commonsense": 47.0,
      "mmlu_overall": 59.175438596491226,
      "humanities": 63.92307692307693,
      "mmlu.formal_logic": 47.0,
      "mmlu.high_school_european_history": 73.0,
      "mmlu.high_school_us_history": 74.0,
      "mmlu.high_school_world_history": 76.0,
      "mmlu.international_law": 75.0,
      "mmlu.jurisprudence": 74.0,
      "mmlu.logical_fallacies": 78.0,
      "mmlu.moral_disputes": 64.0,
      "mmlu.moral_scenarios": 28.000000000000004,
      "mmlu.philosophy": 65.0,
      "mmlu.prehistory": 64.0,
      "mmlu.professional_law": 39.0,
      "mmlu.world_religions": 74.0,
      "other": 62.0,
      "mmlu.business_ethics": 61.0,
      "mmlu.clinical_knowledge": 68.0,
      "mmlu.college_medicine": 64.0,
      "mmlu.global_facts": 26.0,
      "mmlu.human_aging": 66.0,
      "mmlu.management": 76.0,
      "mmlu.marketing": 86.0,
      "mmlu.medical_genetics": 67.0,
      "mmlu.miscellaneous": 64.0,
      "mmlu.nutrition": 69.0,
      "mmlu.professional_accounting": 49.0,
      "mmlu.professional_medicine": 66.0,
      "mmlu.virology": 44.0,
      "social_sciences": 68.75,
      "mmlu.econometrics": 47.0,
      "mmlu.high_school_geography": 78.0,
      "mmlu.high_school_government_and_politics": 79.0,
      "mmlu.high_school_macroeconomics": 65.0,
      "mmlu.high_school_microeconomics": 71.0,
      "mmlu.high_school_psychology": 83.0,
      "mmlu.human_sexuality": 71.0,
      "mmlu.professional_psychology": 60.0,
      "mmlu.public_relations": 62.0,
      "mmlu.security_studies": 62.0,
      "mmlu.sociology": 74.0,
      "mmlu.us_foreign_policy": 73.0,
      "stem": 47.94736842105263,
      "mmlu.abstract_algebra": 26.0,
      "mmlu.anatomy": 51.0,
      "mmlu.astronomy": 68.0,
      "mmlu.college_biology": 63.0,
      "mmlu.college_chemistry": 37.0,
      "mmlu.college_computer_science": 50.0,
      "mmlu.college_mathematics": 28.999999999999996,
      "mmlu.college_physics": 38.0,
      "mmlu.computer_security": 72.0,
      "mmlu.conceptual_physics": 56.99999999999999,
      "mmlu.electrical_engineering": 55.00000000000001,
      "mmlu.elementary_mathematics": 37.0,
      "mmlu.high_school_biology": 70.0,
      "mmlu.high_school_chemistry": 44.0,
      "mmlu.high_school_computer_science": 60.0,
      "mmlu.high_school_mathematics": 33.0,
      "mmlu.high_school_physics": 31.0,
      "mmlu.high_school_statistics": 43.0,
      "mmlu.machine_learning": 47.0
    }
  },
  "table": [
    {
      "model": "qwen/Qwen2.5-3B-Instruct",
      "parameters": "",
      "family": "Qwen",
      "release_date": "",
      "commonsense": 52.0,
      "humanities": 69.308,
      "math_reasoning": 62.0,
      "mmlu.abstract_algebra": 40.0,
      "mmlu.anatomy": 61.0,
      "mmlu.astronomy": 70.0,
      "mmlu.business_ethics": 70.0,
      "mmlu.clinical_knowledge": 69.0,
      "mmlu.college_biology": 73.0,
      "mmlu.college_chemistry": 47.0,
      "mmlu.college_computer_science": 52.0,
      "mmlu.college_mathematics": 35.0,
      "mmlu.college_medicine": 68.0,
      "mmlu.college_physics": 45.0,
      "mmlu.computer_security": 71.0,
      "mmlu.conceptual_physics": 62.0,
      "mmlu.econometrics": 57.0,
      "mmlu.electrical_engineering": 62.0,
      "mmlu.elementary_mathematics": 63.0,
      "mmlu.formal_logic": 45.0,
      "mmlu.global_facts": 40.0,
      "mmlu.high_school_biology": 77.0,
      "mmlu.high_school_chemistry": 61.0,
      "mmlu.high_school_computer_science": 71.0,
      "mmlu.high_school_european_history": 75.0,
      "mmlu.high_school_geography": 74.0,
      "mmlu.high_school_government_and_politics": 86.0,
      "mmlu.high_school_macroeconomics": 74.0,
      "mmlu.high_school_mathematics": 40.0,
      "mmlu.high_school_microeconomics": 76.0,
      "mmlu.high_school_physics": 43.0,
      "mmlu.high_school_psychology": 84.0,
      "mmlu.high_school_statistics": 56.0,
      "mmlu.high_school_us_history": 86.0,
      "mmlu.high_school_world_history": 80.0,
      "mmlu.human_aging": 65.0,
      "mmlu.human_sexuality": 73.0,
      "mmlu.international_law": 76.0,
      "mmlu.jurisprudence": 80.0,
      "mmlu.logical_fallacies": 77.0,
      "mmlu.machine_learning": 45.0,
      "mmlu.management": 78.0,
      "mmlu.marketing": 85.0,
      "mmlu.medical_genetics": 78.0,
      "mmlu.miscellaneous": 82.0,
      "mmlu.moral_disputes": 64.0,
      "mmlu.moral_scenarios": 40.0,
      "mmlu.nutrition": 78.0,
      "mmlu.philosophy": 73.0,
      "mmlu.prehistory": 76.0,
      "mmlu.professional_accounting": 49.0,
      "mmlu.professional_law": 48.0,
      "mmlu.professional_medicine": 71.0,
      "mmlu.professional_psychology": 66.0,
      "mmlu.public_relations": 68.0,
      "mmlu.security_studies": 79.0,
      "mmlu.sociology": 76.0,
      "mmlu.us_foreign_policy": 82.0,
      "mmlu.virology": 53.0,
      "mmlu.world_religions": 81.0,
      "mmlu_overall": 65.895,
      "other": 68.154,
      "reading": 77.0,
      "social_sciences": 74.583,
      "stem": 56.526,
      "overall_score": 65.869,
      "tier": "expert",
      "cluster": 2,
      "cluster_name": "C1"
    },
    {
      "model": "deepseek-ai/DeepSeek-Coder-1.3B-Instruct",
      "parameters": "",
      "family": "DeepSeek",
      "release_date": "",
      "commonsense": 36.0,
      "humanities": 25.077,
      "math_reasoning": 8.0,
      "mmlu.abstract_algebra": 22.0,
      "mmlu.anatomy": 17.0,
      "mmlu.astronomy": 20.0,
      "mmlu.business_ethics": 32.0,
      "mmlu.clinical_knowledge": 15.0,
      "mmlu.college_biology": 24.0,
      "mmlu.college_chemistry": 20.0,
      "mmlu.college_computer_science": 36.0,
      "mmlu.college_mathematics": 22.0,
      "mmlu.college_medicine": 18.0,
      "mmlu.college_physics": 20.0,
      "mmlu.computer_security": 34.0,
      "mmlu.conceptual_physics": 33.0,
      "mmlu.econometrics": 21.0,
      "mmlu.electrical_engineering": 29.0,
      "mmlu.elementary_mathematics": 26.0,
      "mmlu.formal_logic": 27.0,
      "mmlu.global_facts": 18.0,
      "mmlu.high_school_biology": 15.0,
      "mmlu.high_school_chemistry": 18.0,
      "mmlu.high_school_computer_science": 29.0,
      "mmlu.high_school_european_history": 23.0,
      "mmlu.high_school_geography": 16.0,
      "mmlu.high_school_government_and_politics": 16.0,
      "mmlu.high_school_macroeconomics": 14.0,
      "mmlu.high_school_mathematics": 23.0,
      "mmlu.high_school_microeconomics": 26.0,
      "mmlu.high_school_physics": 23.0,
      "mmlu.high_school_psychology": 23.0,
      "mmlu.high_school_statistics": 21.0,
      "mmlu.high_school_us_history": 25.0,
      "mmlu.high_school_world_history": 30.0,
      "mmlu.human_aging": 29.0,
      "mmlu.human_sexuality": 23.0,
      "mmlu.international_law": 25.0,
      "mmlu.jurisprudence": 28.0,
      "mmlu.logical_fallacies": 24.0,
      "mmlu.machine_learning": 32.0,
      "mmlu.management": 16.0,
      "mmlu.marketing": 29.0,
      "mmlu.medical_genetics": 29.0,
      "mmlu.miscellaneous": 19.0,
      "mmlu.moral_disputes": 22.0,
      "mmlu.moral_scenarios": 22.0,
      "mmlu.nutrition": 21.0,
      "mmlu.philosophy": 15.0,
      "mmlu.prehistory": 23.0,
      "mmlu.professional_accounting": 21.0,
      "mmlu.professional_law": 22.0,
      "mmlu.professional_medicine": 15.0,
      "mmlu.professional_psychology": 27.0,
      "mmlu.public_relations": 24.0,
      "mmlu.security_studies": 27.0,
      "mmlu.sociology": 33.0,
      "mmlu.us_foreign_policy": 28.0,
      "mmlu.virology": 27.0,
      "mmlu.world_religions": 40.0,
      "mmlu_overall": 23.807,
      "other": 22.231,
      "reading": 71.0,
      "social_sciences": 23.167,
      "stem": 24.421,
      "overall_score": 24.472,
      "tier": "expert",
      "cluster": 0,
      "cluster_name": "C3"
    },
    {
      "model": "qwen/Qwen2.5-1.5B-Instruct",
      "parameters": "",
      "family": "Qwen",
      "release_date": "",
      "commonsense": 47.0,
      "humanities": 63.923,
      "math_reasoning": 57.0,
      "mmlu.abstract_algebra": 26.0,
      "mmlu.anatomy": 51.0,
      "mmlu.astronomy": 68.0,
      "mmlu.business_ethics": 61.0,
      "mmlu.clinical_knowledge": 68.0,
      "mmlu.college_biology": 63.0,
      "mmlu.college_chemistry": 37.0,
      "mmlu.college_computer_science": 50.0,
      "mmlu.college_mathematics": 29.0,
      "mmlu.college_medicine": 64.0,
      "mmlu.college_physics": 38.0,
      "mmlu.computer_security": 72.0,
      "mmlu.conceptual_physics": 57.0,
      "mmlu.econometrics": 47.0,
      "mmlu.electrical_engineering": 55.0,
      "mmlu.elementary_mathematics": 37.0,
      "mmlu.formal_logic": 47.0,
      "mmlu.global_facts": 26.0,
      "mmlu.high_school_biology": 70.0,
      "mmlu.high_school_chemistry": 44.0,
      "mmlu.high_school_computer_science": 60.0,
      "mmlu.high_school_european_history": 73.0,
      "mmlu.high_school_geography": 78.0,
      "mmlu.high_school_government_and_politics": 79.0,
      "mmlu.high_school_macroeconomics": 65.0,
      "mmlu.high_school_mathematics": 33.0,
      "mmlu.high_school_microeconomics": 71.0,
      "mmlu.high_school_physics": 31.0,
      "mmlu.high_school_psychology": 83.0,
      "mmlu.high_school_statistics": 43.0,
      "mmlu.high_school_us_history": 74.0,
      "mmlu.high_school_world_history": 76.0,
      "mmlu.human_aging": 66.0,
      "mmlu.human_sexuality": 71.0,
      "mmlu.international_law": 75.0,
      "mmlu.jurisprudence": 74.0,
      "mmlu.logical_fallacies": 78.0,
      "mmlu.machine_learning": 47.0,
      "mmlu.management": 76.0,
      "mmlu.marketing": 86.0,
      "mmlu.medical_genetics": 67.0,
      "mmlu.miscellaneous": 64.0,
      "mmlu.moral_disputes": 64.0,
      "mmlu.moral_scenarios": 28.0,
      "mmlu.nutrition": 69.0,
      "mmlu.philosophy": 65.0,
      "mmlu.prehistory": 64.0,
      "mmlu.professional_accounting": 49.0,
      "mmlu.professional_law": 39.0,
      "mmlu.professional_medicine": 66.0,
      "mmlu.professional_psychology": 60.0,
      "mmlu.public_relations": 62.0,
      "mmlu.security_studies": 62.0,
      "mmlu.sociology": 74.0,
      "mmlu.us_foreign_policy": 73.0,
      "mmlu.virology": 44.0,
      "mmlu.world_religions": 74.0,
      "mmlu_overall": 59.175,
      "other": 62.0,
      "reading": 79.0,
      "social_sciences": 68.75,
      "stem": 47.947,
      "overall_score": 59.351,
      "tier": "expert",
      "cluster": 1,
      "cluster_name": "C2"
    }
  ]
}