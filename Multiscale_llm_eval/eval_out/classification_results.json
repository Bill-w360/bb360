[
  {
    "model":"qwen\/Qwen2.5-3B-Instruct",
    "parameters":null,
    "family":"Qwen",
    "release_date":null,
    "commonsense":5.024,
    "humanities":1.22,
    "math_reasoning":4.648,
    "mmlu.abstract_algebra":4.924,
    "mmlu.anatomy":4.902,
    "mmlu.astronomy":4.606,
    "mmlu.business_ethics":4.606,
    "mmlu.clinical_knowledge":4.648,
    "mmlu.college_biology":4.462,
    "mmlu.college_chemistry":5.016,
    "mmlu.college_computer_science":5.021,
    "mmlu.college_mathematics":4.794,
    "mmlu.college_medicine":4.688,
    "mmlu.college_physics":5.0,
    "mmlu.computer_security":4.56,
    "mmlu.conceptual_physics":4.878,
    "mmlu.econometrics":4.976,
    "mmlu.electrical_engineering":4.878,
    "mmlu.elementary_mathematics":4.852,
    "mmlu.formal_logic":5.0,
    "mmlu.global_facts":4.924,
    "mmlu.high_school_biology":4.23,
    "mmlu.high_school_chemistry":4.902,
    "mmlu.high_school_computer_science":4.56,
    "mmlu.high_school_european_history":4.352,
    "mmlu.high_school_geography":4.408,
    "mmlu.high_school_government_and_politics":3.487,
    "mmlu.high_school_macroeconomics":4.408,
    "mmlu.high_school_mathematics":4.924,
    "mmlu.high_school_microeconomics":4.292,
    "mmlu.high_school_physics":4.976,
    "mmlu.high_school_psychology":3.685,
    "mmlu.high_school_statistics":4.989,
    "mmlu.high_school_us_history":3.487,
    "mmlu.high_school_world_history":4.02,
    "mmlu.human_aging":4.794,
    "mmlu.human_sexuality":4.462,
    "mmlu.international_law":4.292,
    "mmlu.jurisprudence":4.02,
    "mmlu.logical_fallacies":4.23,
    "mmlu.machine_learning":5.0,
    "mmlu.management":4.163,
    "mmlu.marketing":3.589,
    "mmlu.medical_genetics":4.163,
    "mmlu.miscellaneous":3.861,
    "mmlu.moral_disputes":4.824,
    "mmlu.moral_scenarios":4.924,
    "mmlu.nutrition":4.163,
    "mmlu.philosophy":4.462,
    "mmlu.prehistory":4.292,
    "mmlu.professional_accounting":5.024,
    "mmlu.professional_law":5.021,
    "mmlu.professional_medicine":4.56,
    "mmlu.professional_psychology":4.761,
    "mmlu.public_relations":4.688,
    "mmlu.security_studies":4.094,
    "mmlu.sociology":4.292,
    "mmlu.us_foreign_policy":3.861,
    "mmlu.virology":5.016,
    "mmlu.world_religions":3.943,
    "mmlu_overall":0.602,
    "other":1.248,
    "reading":4.23,
    "social_sciences":1.243,
    "stem":1.106,
    "overall_score":4.266,
    "tier":"expert",
    "cluster":2,
    "cluster_name":"C2"
  },
  {
    "model":"deepseek-ai\/DeepSeek-Coder-1.3B-Instruct",
    "parameters":null,
    "family":"DeepSeek",
    "release_date":null,
    "commonsense":4.96,
    "humanities":1.198,
    "math_reasoning":2.564,
    "mmlu.abstract_algebra":4.163,
    "mmlu.anatomy":3.775,
    "mmlu.astronomy":4.02,
    "mmlu.business_ethics":4.688,
    "mmlu.clinical_knowledge":3.589,
    "mmlu.college_biology":4.292,
    "mmlu.college_chemistry":4.02,
    "mmlu.college_computer_science":4.824,
    "mmlu.college_mathematics":4.163,
    "mmlu.college_medicine":3.861,
    "mmlu.college_physics":4.02,
    "mmlu.computer_security":4.761,
    "mmlu.conceptual_physics":4.726,
    "mmlu.econometrics":4.094,
    "mmlu.electrical_engineering":4.56,
    "mmlu.elementary_mathematics":4.408,
    "mmlu.formal_logic":4.462,
    "mmlu.global_facts":3.861,
    "mmlu.high_school_biology":3.589,
    "mmlu.high_school_chemistry":3.861,
    "mmlu.high_school_computer_science":4.56,
    "mmlu.high_school_european_history":4.23,
    "mmlu.high_school_geography":3.685,
    "mmlu.high_school_government_and_politics":3.685,
    "mmlu.high_school_macroeconomics":3.487,
    "mmlu.high_school_mathematics":4.23,
    "mmlu.high_school_microeconomics":4.408,
    "mmlu.high_school_physics":4.23,
    "mmlu.high_school_psychology":4.23,
    "mmlu.high_school_statistics":4.094,
    "mmlu.high_school_us_history":4.352,
    "mmlu.high_school_world_history":4.606,
    "mmlu.human_aging":4.56,
    "mmlu.human_sexuality":4.23,
    "mmlu.international_law":4.352,
    "mmlu.jurisprudence":4.513,
    "mmlu.logical_fallacies":4.292,
    "mmlu.machine_learning":4.688,
    "mmlu.management":3.685,
    "mmlu.marketing":4.56,
    "mmlu.medical_genetics":4.56,
    "mmlu.miscellaneous":3.943,
    "mmlu.moral_disputes":4.163,
    "mmlu.moral_scenarios":4.163,
    "mmlu.nutrition":4.094,
    "mmlu.philosophy":3.589,
    "mmlu.prehistory":4.23,
    "mmlu.professional_accounting":4.094,
    "mmlu.professional_law":4.163,
    "mmlu.professional_medicine":3.589,
    "mmlu.professional_psychology":4.462,
    "mmlu.public_relations":4.292,
    "mmlu.security_studies":4.462,
    "mmlu.sociology":4.726,
    "mmlu.us_foreign_policy":4.513,
    "mmlu.virology":4.462,
    "mmlu.world_religions":4.924,
    "mmlu_overall":0.562,
    "other":1.147,
    "reading":4.56,
    "social_sciences":1.214,
    "stem":0.981,
    "overall_score":3.97,
    "tier":"competent",
    "cluster":0,
    "cluster_name":"C3"
  },
  {
    "model":"qwen\/Qwen2.5-1.5B-Instruct",
    "parameters":null,
    "family":"Qwen",
    "release_date":null,
    "commonsense":4.96,
    "humanities":1.269,
    "math_reasoning":4.924,
    "mmlu.abstract_algebra":4.408,
    "mmlu.anatomy":5.024,
    "mmlu.astronomy":4.688,
    "mmlu.business_ethics":4.902,
    "mmlu.clinical_knowledge":4.688,
    "mmlu.college_biology":4.852,
    "mmlu.college_chemistry":4.852,
    "mmlu.college_computer_science":5.025,
    "mmlu.college_mathematics":4.56,
    "mmlu.college_medicine":4.824,
    "mmlu.college_physics":4.878,
    "mmlu.computer_security":4.513,
    "mmlu.conceptual_physics":4.976,
    "mmlu.econometrics":5.016,
    "mmlu.electrical_engineering":5.0,
    "mmlu.elementary_mathematics":4.852,
    "mmlu.formal_logic":5.016,
    "mmlu.global_facts":4.408,
    "mmlu.high_school_biology":4.606,
    "mmlu.high_school_chemistry":4.989,
    "mmlu.high_school_computer_science":4.924,
    "mmlu.high_school_european_history":4.462,
    "mmlu.high_school_geography":4.163,
    "mmlu.high_school_government_and_politics":4.094,
    "mmlu.high_school_macroeconomics":4.794,
    "mmlu.high_school_mathematics":4.726,
    "mmlu.high_school_microeconomics":4.56,
    "mmlu.high_school_physics":4.648,
    "mmlu.high_school_psychology":3.775,
    "mmlu.high_school_statistics":4.976,
    "mmlu.high_school_us_history":4.408,
    "mmlu.high_school_world_history":4.292,
    "mmlu.human_aging":4.761,
    "mmlu.human_sexuality":4.56,
    "mmlu.international_law":4.352,
    "mmlu.jurisprudence":4.408,
    "mmlu.logical_fallacies":4.163,
    "mmlu.machine_learning":5.016,
    "mmlu.management":4.292,
    "mmlu.marketing":3.487,
    "mmlu.medical_genetics":4.726,
    "mmlu.miscellaneous":4.824,
    "mmlu.moral_disputes":4.824,
    "mmlu.moral_scenarios":4.513,
    "mmlu.nutrition":4.648,
    "mmlu.philosophy":4.794,
    "mmlu.prehistory":4.824,
    "mmlu.professional_accounting":5.024,
    "mmlu.professional_law":4.902,
    "mmlu.professional_medicine":4.761,
    "mmlu.professional_psychology":4.924,
    "mmlu.public_relations":4.878,
    "mmlu.security_studies":4.878,
    "mmlu.sociology":4.408,
    "mmlu.us_foreign_policy":4.462,
    "mmlu.virology":4.989,
    "mmlu.world_religions":4.408,
    "mmlu_overall":0.619,
    "other":1.292,
    "reading":4.094,
    "social_sciences":1.316,
    "stem":1.106,
    "overall_score":4.389,
    "tier":"expert",
    "cluster":1,
    "cluster_name":"C1"
  }
]